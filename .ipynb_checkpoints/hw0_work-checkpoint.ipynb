{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19f04c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "import numdifftools as nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "050e853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mnist(image_filename, label_filename):\n",
    "    \"\"\" Read an images and labels file in MNIST format.  See this page:\n",
    "    http://yann.lecun.com/exdb/mnist/ for a description of the file format.\n",
    "\n",
    "    Args:\n",
    "        image_filename (str): name of gzipped images file in MNIST format\n",
    "        label_filename (str): name of gzipped labels file in MNIST format\n",
    "\n",
    "    Returns:\n",
    "        Tuple (X,y):\n",
    "            X (numpy.ndarray[np.float32]): 2D numpy array containing the loaded \n",
    "                data.  The dimensionality of the data should be \n",
    "                (num_examples x input_dim) where 'input_dim' is the full \n",
    "                dimension of the data, e.g., since MNIST images are 28x28, it \n",
    "                will be 784.  Values should be of type np.float32, and the data \n",
    "                should be normalized to have a minimum value of 0.0 and a \n",
    "                maximum value of 1.0. The normalization should be applied uniformly\n",
    "                across the whole dataset, _not_ individual images.\n",
    "\n",
    "            y (numpy.ndarray[dtype=np.uint8]): 1D numpy array containing the\n",
    "                labels of the examples.  Values should be of type np.uint8 and\n",
    "                for MNIST will contain the values 0-9.\n",
    "    \"\"\"\n",
    "    ### BEGIN YOUR CODE\n",
    "    def readLabels(filePath=label_filename):\n",
    "        with gzip.open(filePath, 'rb') as f:\n",
    "            return [struct.unpack('>II', f.read(8)),np.frombuffer(f.read(),dtype=np.uint8)]\n",
    "    def readImages(filePath=image_filename):\n",
    "        with gzip.open(filePath, 'rb') as f:\n",
    "            [magic,images,rows,cols]=struct.unpack('>IIII', f.read(16))\n",
    "            return np.resize(np.frombuffer(f.read(),dtype=np.uint8),(images,rows*cols))/np.float32(255)\n",
    "    return (readImages(),readLabels()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b9a6ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(Z, y):\n",
    "    \"\"\" Return softmax loss.  Note that for the purposes of this assignment,\n",
    "    you don't need to worry about \"nicely\" scaling the numerical properties\n",
    "    of the log-sum-exp computation, but can just compute this directly.\n",
    "\n",
    "    Args:\n",
    "        Z (np.ndarray[np.float32]): 2D numpy array of shape\n",
    "            (batch_size, num_classes), containing the logit predictions for\n",
    "            each class.\n",
    "        y (np.ndarray[np.int8]): 1D numpy array of shape (batch_size, )\n",
    "            containing the true label of each example.\n",
    "\n",
    "    Returns:\n",
    "        Average softmax loss over the sample.\n",
    "    \"\"\"\n",
    "    ### BEGIN YOUR CODE\n",
    "    return np.average([*map(lambda idx:np.log(np.sum(np.exp(Z[idx])))-Z[idx][y[idx]],[x for x in range(len(y))])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d37c661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_regression_epoch(X, y, theta, lr = 0.1, batch=100):\n",
    "    \"\"\" Run a single epoch of SGD for softmax regression on the data, using\n",
    "    the step size lr and specified batch size.  This function should modify the\n",
    "    theta matrix in place, and you should iterate through batches in X _without_\n",
    "    randomizing the order.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray[np.float32]): 2D input array of size\n",
    "            (num_examples x input_dim).\n",
    "        y (np.ndarray[np.uint8]): 1D class label array of size (num_examples,)\n",
    "        theta (np.ndarrray[np.float32]): 2D array of softmax regression\n",
    "            parameters, of shape (input_dim, num_classes)\n",
    "        lr (float): step size (learning rate) for SGD\n",
    "        batch (int): size of SGD minibatch\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    ### BEGIN YOUR CODE\n",
    "    def normalize(cur):\n",
    "        def calCuri(curi): \n",
    "            return np.exp(curi)/np.sum(np.exp(cur))\n",
    "        return [*map(calCuri,cur)]\n",
    "    for index in range(0,len(y),batch):\n",
    "        batchX,batchy=X[index:index+batch],y[index:index+batch]\n",
    "        Z=np.array([*map(normalize,np.dot(batchX,theta))])\n",
    "        newy=np.array([np.concatenate((x[0:yy],np.array([1]),x[yy+1:]), axis=0) \n",
    "          for (x,yy) in zip(np.zeros(shape=(batchy.shape[0],theta[0].shape[0])),batchy)\n",
    "         ])\n",
    "        theta-=(lr/batch)*np.dot(batchX.transpose(),(Z-newy))\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb226168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_epoch(X, y, W1, W2, lr = 0.1, batch=100):\n",
    "    \"\"\" Run a single epoch of SGD for a two-layer neural network defined by the\n",
    "    weights W1 and W2 (with no bias terms):\n",
    "        logits = ReLU(X * W1) * W2\n",
    "    The function should use the step size lr, and the specified batch size (and\n",
    "    again, without randomizing the order of X).  It should modify the\n",
    "    W1 and W2 matrices in place.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray[np.float32]): 2D input array of size\n",
    "            (num_examples x input_dim).\n",
    "        y (np.ndarray[np.uint8]): 1D class label array of size (num_examples,)\n",
    "        W1 (np.ndarray[np.float32]): 2D array of first layer weights, of shape\n",
    "            (input_dim, hidden_dim)\n",
    "        W2 (np.ndarray[np.float32]): 2D array of second layer weights, of shape\n",
    "            (hidden_dim, num_classes)\n",
    "        lr (float): step size (learning rate) for SGD\n",
    "        batch (int): size of SGD minibatch\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    ### BEGIN YOUR CODE\n",
    "    def RELU(x):\n",
    "        def relu(x):\n",
    "            return max(0,x)\n",
    "        return np.array(np.frompyfunc(relu,1,1)(x),dtype=np.float32)\n",
    "    def normalize(cur):\n",
    "        def calCuri(curi): \n",
    "            return np.exp(curi)/np.sum(np.exp(cur))\n",
    "        return np.apply_along_axis(calCuri,0,cur)\n",
    "    def NOTZERO(x):\n",
    "        def isZero(x):\n",
    "            return 1 if x>0 else 0\n",
    "        return np.array(np.frompyfunc(isZero,1,1)(x),dtype=np.float32)\n",
    "    for index in range(0,len(y),batch):\n",
    "        batchX,batchy=X[index:index+batch],y[index:index+batch]\n",
    "        Iy=np.array([np.concatenate((x[0:yy],np.array([1]),x[yy+1:]), axis=0) \n",
    "          for (x,yy) in zip(np.zeros(shape=(batchy.shape[0],W2.shape[1])),batchy)\n",
    "         ])\n",
    "        Z1=RELU(np.dot(batchX,W1))\n",
    "        G2=np.apply_along_axis(normalize,1,np.dot(Z1,W2))-Iy\n",
    "        G1=np.multiply(np.where(Z1) > 0, 1, 0),(np.dot(G2,W2.transpose())))\n",
    "        W1-=(lr/batch)*np.dot(batchX.transpose(),G1)\n",
    "        W2-=(lr/batch)*np.dot(Z1.transpose(),G2)\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46912c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(X_tr, y_tr, X_te, y_te, hidden_dim = 500,\n",
    "             epochs=10, lr=0.5, batch=100):\n",
    "    \"\"\" Example function to train two layer neural network \"\"\"\n",
    "    n, k = X_tr.shape[1], y_tr.max() + 1\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(n, hidden_dim).astype(np.float32) / np.sqrt(hidden_dim)\n",
    "    W2 = np.random.randn(hidden_dim, k).astype(np.float32) / np.sqrt(k)\n",
    "\n",
    "    print(\"| Epoch | Train Loss | Train Err | Test Loss | Test Err |\")\n",
    "    for epoch in range(epochs):\n",
    "        nn_epoch(X_tr, y_tr, W1, W2, lr=lr, batch=batch)\n",
    "        train_loss, train_err = loss_err(np.maximum(X_tr@W1,0)@W2, y_tr)\n",
    "        test_loss, test_err = loss_err(np.maximum(X_te@W1,0)@W2, y_te)\n",
    "        print(\"|  {:>4} |    {:.5f} |   {:.5f} |   {:.5f} |  {:.5f} |\"\\\n",
    "              .format(epoch, train_loss, train_err, test_loss, test_err))\n",
    "def loss_err(h,y):\n",
    "    \"\"\" Helper funciton to compute both loss and error\"\"\"\n",
    "    return softmax_loss(h,y), np.mean(h.argmax(axis=1) != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99809b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch | Train Loss | Train Err | Test Loss | Test Err |\n",
      "|     0 |    0.15324 |   0.04697 |   0.16305 |  0.04920 |\n",
      "|     1 |    0.09854 |   0.02923 |   0.11604 |  0.03660 |\n",
      "|     2 |    0.07429 |   0.02168 |   0.09774 |  0.03160 |\n",
      "|     3 |    0.05959 |   0.01732 |   0.08790 |  0.02930 |\n",
      "|     4 |    0.04820 |   0.01348 |   0.08064 |  0.02610 |\n"
     ]
    }
   ],
   "source": [
    "X_tr, y_tr = parse_mnist(\"data/train-images-idx3-ubyte.gz\", \n",
    "                         \"data/train-labels-idx1-ubyte.gz\")\n",
    "X_te, y_te = parse_mnist(\"data/t10k-images-idx3-ubyte.gz\",\n",
    "                         \"data/t10k-labels-idx1-ubyte.gz\")\n",
    "train_nn(X_tr, y_tr, X_te, y_te, hidden_dim=400, epochs=20, lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e65946a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where( np.array([1, -2, 3, 0, -4, 5]) > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef786bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
