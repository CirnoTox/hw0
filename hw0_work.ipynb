{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19f04c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "import numdifftools as nd\n",
    "def softmax_loss(Z, y):\n",
    "    return np.average([*map(lambda idx:np.log(np.sum(np.exp(Z[idx])))-Z[idx][y[idx]],[x for x in range(len(y))])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fe00a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X = np.random.randn(10,5).astype(np.float32)\n",
    "y = np.random.randint(3, size=(10,)).astype(np.uint8)\n",
    "Theta = np.zeros((5,3), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06e773e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 1 1 1 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "050e853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mnist(image_filename, label_filename):\n",
    "    \"\"\" Read an images and labels file in MNIST format.  See this page:\n",
    "    http://yann.lecun.com/exdb/mnist/ for a description of the file format.\n",
    "\n",
    "    Args:\n",
    "        image_filename (str): name of gzipped images file in MNIST format\n",
    "        label_filename (str): name of gzipped labels file in MNIST format\n",
    "\n",
    "    Returns:\n",
    "        Tuple (X,y):\n",
    "            X (numpy.ndarray[np.float32]): 2D numpy array containing the loaded \n",
    "                data.  The dimensionality of the data should be \n",
    "                (num_examples x input_dim) where 'input_dim' is the full \n",
    "                dimension of the data, e.g., since MNIST images are 28x28, it \n",
    "                will be 784.  Values should be of type np.float32, and the data \n",
    "                should be normalized to have a minimum value of 0.0 and a \n",
    "                maximum value of 1.0. The normalization should be applied uniformly\n",
    "                across the whole dataset, _not_ individual images.\n",
    "\n",
    "            y (numpy.ndarray[dtype=np.uint8]): 1D numpy array containing the\n",
    "                labels of the examples.  Values should be of type np.uint8 and\n",
    "                for MNIST will contain the values 0-9.\n",
    "    \"\"\"\n",
    "    ### BEGIN YOUR CODE\n",
    "    def readLabels(filePath=label_filename):\n",
    "        with gzip.open(filePath, 'rb') as f:\n",
    "            return [struct.unpack('>II', f.read(8)),np.frombuffer(f.read(),dtype=np.uint8)]\n",
    "    def readImages(filePath=image_filename):\n",
    "        with gzip.open(filePath, 'rb') as f:\n",
    "            [magic,images,rows,cols]=struct.unpack('>IIII', f.read(16))\n",
    "            return np.resize(np.frombuffer(f.read(),dtype=np.uint8),(images,rows*cols))/np.float32(255)\n",
    "    return (readImages(),readLabels()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b9a6ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(Z, y):\n",
    "    \"\"\" Return softmax loss.  Note that for the purposes of this assignment,\n",
    "    you don't need to worry about \"nicely\" scaling the numerical properties\n",
    "    of the log-sum-exp computation, but can just compute this directly.\n",
    "\n",
    "    Args:\n",
    "        Z (np.ndarray[np.float32]): 2D numpy array of shape\n",
    "            (batch_size, num_classes), containing the logit predictions for\n",
    "            each class.\n",
    "        y (np.ndarray[np.int8]): 1D numpy array of shape (batch_size, )\n",
    "            containing the true label of each example.\n",
    "\n",
    "    Returns:\n",
    "        Average softmax loss over the sample.\n",
    "    \"\"\"\n",
    "    ### BEGIN YOUR CODE\n",
    "    return np.average([*map(lambda idx:np.log(np.sum(np.exp(Z[idx])))-Z[idx][y[idx]],[x for x in range(len(y))])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d37c661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_regression_epoch(X, y, theta, lr = 0.1, batch=100):\n",
    "    \"\"\" Run a single epoch of SGD for softmax regression on the data, using\n",
    "    the step size lr and specified batch size.  This function should modify the\n",
    "    theta matrix in place, and you should iterate through batches in X _without_\n",
    "    randomizing the order.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray[np.float32]): 2D input array of size\n",
    "            (num_examples x input_dim).\n",
    "        y (np.ndarray[np.uint8]): 1D class label array of size (num_examples,)\n",
    "        theta (np.ndarrray[np.float32]): 2D array of softmax regression\n",
    "            parameters, of shape (input_dim, num_classes)\n",
    "        lr (float): step size (learning rate) for SGD\n",
    "        batch (int): size of SGD minibatch\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    ### BEGIN YOUR CODE\n",
    "    def normalize(cur):\n",
    "        def calCuri(curi): \n",
    "            return np.exp(curi)/np.sum(np.exp(cur))\n",
    "        return [*map(calCuri,cur)]\n",
    "    for index in range(0,len(y),batch):\n",
    "        batchX,batchy=X[index:index+batch],y[index:index+batch]\n",
    "        Z=np.array([*map(normalize,np.dot(batchX,theta))])\n",
    "        newy=np.array([np.concatenate((x[0:yy],np.array([1]),x[yy+1:]), axis=0) \n",
    "          for (x,yy) in zip(np.zeros(shape=(batchy.shape[0],theta[0].shape[0])),batchy)\n",
    "         ])\n",
    "        theta-=(lr/batch)*np.dot(batchX.transpose(),(Z-newy))\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb226168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_epoch(X, y, W1, W2, lr = 0.1, batch=100):\n",
    "    \"\"\" Run a single epoch of SGD for a two-layer neural network defined by the\n",
    "    weights W1 and W2 (with no bias terms):\n",
    "        logits = ReLU(X * W1) * W2\n",
    "    The function should use the step size lr, and the specified batch size (and\n",
    "    again, without randomizing the order of X).  It should modify the\n",
    "    W1 and W2 matrices in place.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray[np.float32]): 2D input array of size\n",
    "            (num_examples x input_dim).\n",
    "        y (np.ndarray[np.uint8]): 1D class label array of size (num_examples,)\n",
    "        W1 (np.ndarray[np.float32]): 2D array of first layer weights, of shape\n",
    "            (input_dim, hidden_dim)\n",
    "        W2 (np.ndarray[np.float32]): 2D array of second layer weights, of shape\n",
    "            (hidden_dim, num_classes)\n",
    "        lr (float): step size (learning rate) for SGD\n",
    "        batch (int): size of SGD minibatch\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    ### BEGIN YOUR CODE\n",
    "    def RELU(arr):\n",
    "        return np.maximum(0, arr)\n",
    "    def softmax_columns(arr):\n",
    "        return np.exp(arr) / np.sum(np.exp(arr), axis=1, keepdims=True)\n",
    "    for index in range(0,len(y),batch):\n",
    "        batchX,batchy=X[index:index+batch],y[index:index+batch]\n",
    "        Iy=np.array([np.concatenate((x[0:yy],np.array([1]),x[yy+1:]), axis=0) \n",
    "          for (x,yy) in zip(np.zeros(shape=(batchy.shape[0],W2.shape[1])),batchy)\n",
    "         ])\n",
    "        Z1=RELU(np.dot(batchX,W1))\n",
    "        G2=softmax_columns(np.dot(Z1,W2))-Iy\n",
    "        G1=np.multiply(np.where(Z1 > 0, 1, 0),(np.dot(G2,W2.transpose())))\n",
    "        W1-=(lr/batch)*np.dot(batchX.transpose(),G1)\n",
    "        W2-=(lr/batch)*np.dot(Z1.transpose(),G2)\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15c77a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(X_tr, y_tr, X_te, y_te, hidden_dim = 500,\n",
    "             epochs=10, lr=0.5, batch=100):\n",
    "    \"\"\" Example function to train two layer neural network \"\"\"\n",
    "    n, k = X_tr.shape[1], y_tr.max() + 1\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(n, hidden_dim).astype(np.float32) / np.sqrt(hidden_dim)\n",
    "    W2 = np.random.randn(hidden_dim, k).astype(np.float32) / np.sqrt(k)\n",
    "\n",
    "    print(\"| Epoch | Train Loss | Train Err | Test Loss | Test Err |\")\n",
    "    for epoch in range(epochs):\n",
    "        nn_epoch(X_tr, y_tr, W1, W2, lr=lr, batch=batch)\n",
    "        train_loss, train_err = loss_err(np.maximum(X_tr@W1,0)@W2, y_tr)\n",
    "        test_loss, test_err = loss_err(np.maximum(X_te@W1,0)@W2, y_te)\n",
    "        print(\"|  {:>4} |    {:.5f} |   {:.5f} |   {:.5f} |  {:.5f} |\"\\\n",
    "              .format(epoch, train_loss, train_err, test_loss, test_err))\n",
    "def loss_err(h,y):\n",
    "    \"\"\" Helper funciton to compute both loss and error\"\"\"\n",
    "    return softmax_loss(h,y), np.mean(h.argmax(axis=1) != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "595f27e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch | Train Loss | Train Err | Test Loss | Test Err |\n",
      "|     0 |    0.15324 |   0.04697 |   0.16305 |  0.04920 |\n",
      "|     1 |    0.09854 |   0.02923 |   0.11604 |  0.03660 |\n",
      "|     2 |    0.07429 |   0.02168 |   0.09774 |  0.03160 |\n",
      "|     3 |    0.05959 |   0.01732 |   0.08790 |  0.02930 |\n",
      "|     4 |    0.04820 |   0.01348 |   0.08064 |  0.02610 |\n",
      "|     5 |    0.04016 |   0.01087 |   0.07663 |  0.02380 |\n",
      "|     6 |    0.03446 |   0.00897 |   0.07410 |  0.02330 |\n",
      "|     7 |    0.03023 |   0.00765 |   0.07252 |  0.02260 |\n",
      "|     8 |    0.02660 |   0.00650 |   0.07101 |  0.02240 |\n",
      "|     9 |    0.02348 |   0.00545 |   0.06992 |  0.02160 |\n",
      "|    10 |    0.02099 |   0.00472 |   0.06905 |  0.02160 |\n",
      "|    11 |    0.01891 |   0.00392 |   0.06836 |  0.02130 |\n",
      "|    12 |    0.01702 |   0.00328 |   0.06769 |  0.02090 |\n",
      "|    13 |    0.01559 |   0.00282 |   0.06728 |  0.02110 |\n",
      "|    14 |    0.01406 |   0.00240 |   0.06667 |  0.02070 |\n",
      "|    15 |    0.01276 |   0.00208 |   0.06602 |  0.02020 |\n",
      "|    16 |    0.01179 |   0.00182 |   0.06572 |  0.01980 |\n",
      "|    17 |    0.01079 |   0.00147 |   0.06541 |  0.01970 |\n",
      "|    18 |    0.00984 |   0.00112 |   0.06490 |  0.01930 |\n",
      "|    19 |    0.00913 |   0.00092 |   0.06474 |  0.01910 |\n"
     ]
    }
   ],
   "source": [
    "X_tr, y_tr = parse_mnist(\"data/train-images-idx3-ubyte.gz\", \n",
    "                         \"data/train-labels-idx1-ubyte.gz\")\n",
    "X_te, y_te = parse_mnist(\"data/t10k-images-idx3-ubyte.gz\",\n",
    "                         \"data/t10k-labels-idx1-ubyte.gz\")\n",
    "train_nn(X_tr, y_tr, X_te, y_te, hidden_dim=400, epochs=20, lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "245a7e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_columns(arr):\n",
    "    return np.exp(arr) / np.sum(np.exp(arr), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88cc1a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09003057, 0.24472847, 0.66524096],\n",
       "       [0.09003057, 0.24472847, 0.66524096]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "softmax_columns(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8dee1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
